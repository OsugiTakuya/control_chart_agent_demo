import datetime
import logging
import os
import re
import subprocess
import sys
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, TypedDict, Union

import streamlit as st
from dotenv import load_dotenv

# ãƒ„ãƒ¼ãƒ«ï¼ˆWebæ¤œç´¢ & Pythonå®Ÿè¡Œï¼‰
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureChatOpenAI, ChatOpenAI

# LangGraph
from langgraph.graph import END, StateGraph

# --- LangChain / OpenAI ---
from pydantic import BaseModel, Field

# =========================
# ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ– â€” ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ãƒˆï¼‰
# =========================
# ç’°å¢ƒå¤‰æ•°ã§ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚„å›è»¢è¨­å®šã‚’ä¸Šæ›¸ãå¯èƒ½
log_dir = Path(os.getenv("LOG_DIR", "logs"))
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / "llm_multi_agent.log"

logger = logging.getLogger("llm_multi_agent")
logger.setLevel(logging.INFO)
# Streamlit ã®ãƒªãƒ­ãƒ¼ãƒ‰ã§ãƒãƒ³ãƒ‰ãƒ©ã‚’é‡è¤‡ã—ã¦è¿½åŠ ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
if not logger.handlers:
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ï¼ˆå¾“æ¥é€šã‚Šï¼‰
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s"))
    logger.addHandler(ch)

    # RotatingFileHandler: maxBytesï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 5MBï¼‰, backupCountï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 3ï¼‰
    try:
        max_bytes = int(os.getenv("LOG_MAX_BYTES", str(5 * 1024 * 1024)))
        backup_count = int(os.getenv("LOG_BACKUP_COUNT", "3"))
        fh = RotatingFileHandler(
            filename=str(log_file),
            maxBytes=max_bytes,
            backupCount=backup_count,
            encoding="utf-8",
        )
        fh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s"))
        logger.addHandler(fh)
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ã®ä½œæˆã«å¤±æ•—ã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã¯ç¶šè¡Œã€‚ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¸ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™ã€‚
        logger.error("Failed to create RotatingFileHandler: %s", e)

    # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã¸ã®ä¼æ’­ã‚’æ­¢ã‚ã¦äºŒé‡ãƒ­ã‚°åŒ–ã‚’é¿ã‘ã‚‹
    logger.propagate = False

# =========================
# Env èª­ã¿è¾¼ã¿ & LLM Factory
# =========================
load_dotenv()

# å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«/ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®šã‚’.envã‹ã‚‰èª­ã¿è¾¼ã¿
AGENT_CONFIG = {
    "router": {
        "model": os.getenv("ROUTER_MODEL"),
        "azure_deployment": os.getenv("ROUTER_AZURE_DEPLOYMENT"),
    },
    "planner": {
        "model": os.getenv("PLANNER_MODEL"),
        "azure_deployment": os.getenv("PLANNER_AZURE_DEPLOYMENT"),
    },
    "executor": {
        "model": os.getenv("EXECUTOR_MODEL"),
        "azure_deployment": os.getenv("EXECUTOR_AZURE_DEPLOYMENT"),
    },
    "summarizer": {
        "model": os.getenv("SUMMARIZER_MODEL"),
        "azure_deployment": os.getenv("SUMMARIZER_AZURE_DEPLOYMENT"),
    },
    "responder": {
        "model": os.getenv("RESPONDER_MODEL"),
        "azure_deployment": os.getenv("CHAT_AZURE_DEPLOYMENT"),
    },
}


def get_llm(agent: str, temperature: float = 0.2):
    cfg = AGENT_CONFIG[agent]
    model = cfg["model"]
    azure_deployment = cfg["azure_deployment"]

    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION")

    if azure_endpoint and azure_key and azure_deployment:
        return AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_key,
            api_version=azure_api_version,
            deployment_name=azure_deployment,
            temperature=temperature,
        )

    return ChatOpenAI(model=model, temperature=temperature)


# =========================
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® agent system prompts
# =========================
tool_description = (
    "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n"
    "- tools.save_similar_series_plot(filepath: str, target_time: str | datetime.datetime, window: int, product: str, parameter: str) -> None\n"
    "  æŒ‡å®šã—ãŸ target_time ã‚’å«ã‚€æ™‚ç³»åˆ—éƒ¨åˆ†ç³»åˆ—ã¨é¡ä¼¼ã—ãŸåŒºé–“ã‚’æ¤œç´¢ã—ã€å›³ã‚’ç”Ÿæˆã—ã¦ filepath ã«ä¿å­˜ã™ã‚‹ã€‚\n"
    "  target_time ã¯æ–‡å­—åˆ—ã¾ãŸã¯ datetimeã€window ã¯ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã€product ã¯è£½å“åã€parameter ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’æŒ‡å®šã€‚\n"
)

DEFAULT_AGENT_PROMPTS = {
    "router": (
        "ã‚ãªãŸã¯å…¥åŠ›ãŒã€é€šå¸¸ã®ä¼šè©±ã‹/ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ã¹ãã‹ã€ã‚’åˆ¤å®šã™ã‚‹åˆ†é¡å™¨ã§ã™ã€‚"
        "ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ»è¨ˆç®—ãƒ»ã‚°ãƒ©ãƒ•ä½œæˆãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€å®Ÿè¡ŒçµæœãŒå¿…è¦ãªå ´åˆã¯ run_code=Trueã€‚"
        "ä¸€èˆ¬çš„ãªQAã‚„è¦ç´„ã€èª¬æ˜ã¯ run_code=Falseã€‚"
    ),
    "planner": (
        "ã‚ãªãŸã¯ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶è¦æ±‚ã‚’æº€ãŸã™ãŸã‚ã®å‡¦ç†æ‰‹é †ã‚’ã€"
        "å®Ÿè¡Œå¯èƒ½ãªPythonã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ã—ã¦çŸ­ã„ãƒªã‚¹ãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"{tool_description}\n"
        "- æ™‚ç³»åˆ—é¡ä¼¼ç®‡æ‰€ã®å¯è¦–åŒ–ãŒç›®çš„ãªã‚‰å¿…ãš save_similar_series_plot ã‚’è¨ˆç”»ã«å«ã‚ã¦ãã ã•ã„ã€‚\n"
        "- å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ç‹¬ç«‹ã—ãŸ .py ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚\n"
        "- å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚"
    ),
    "executor_1": (
        "ã‚ãªãŸã¯Pythonã‚³ãƒ¼ãƒ€ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’æº€ãŸã™æœ€å°ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰ã‚’ã€"
        "1ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯ print ã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚"
        "ãƒ„ãƒ¼ãƒ«ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« tools ã¯æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n"
        "å›³ã‚’ç”Ÿæˆã™ã‚‹å ´åˆã®ãƒ«ãƒ¼ãƒ«:\n"
        "- matplotlib ç­‰ã§ä½œæˆã—ãŸå ´åˆã€å¿…ãš '{image_path_str}' ã«ä¿å­˜ã—ã€"
        "ç›´å¾Œã« `IMAGE_SAVED: {image_path_str}` ã‚’ print ã—ã¦ãã ã•ã„ã€‚\n"
        "- ä¿å­˜å¾Œã¯ plt.close() ã‚’å‘¼ã‚“ã§ãã ã•ã„ã€‚\n"
        "- ç›´æ¥ã®è¨ˆç®—ã‚„ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãªã‚‰é€šå¸¸é€šã‚Š print ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n\n"
        f"{tool_description}"
    ),
    "executor_2": "ã“ã‚Œã¾ã§ã®å®Ÿè¡Œãƒ­ã‚°:\n{previous_execs_text}",
    "summarizer_1": (
        "ã‚ãªãŸã¯ã‚µãƒãƒ©ã‚¤ã‚¶ã§ã™ã€‚ä»¥ä¸‹ã®å®Ÿè¡Œãƒ­ã‚°ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ã®è¦æ±‚ã«å¯¾ã™ã‚‹çµæœã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
        "å¿…è¦ãªã‚‰å®Ÿè¡Œå€¤ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚"
    ),
    "summarizer_2": "å…ƒã®è¦æ±‚: {user_input}",
    "responder_chat": (
        "ã‚ãªãŸã¯æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å¯¾ã—ã¦ã€"
        "ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        "å¿…è¦ãªã‚‰ç®‡æ¡æ›¸ãã‚„å…·ä½“ä¾‹ã‚’ä½¿ã£ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚"
    ),
    "responder_code_1": (
        "ã‚ãªãŸã¯å›ç­”è€…ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å¯¾ã—ã¦ã€"
        "ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        "ä½™è¨ˆãªãƒ¡ã‚¿æƒ…å ±ï¼ˆå®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°ã‚„ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ç­‰ï¼‰ã¯å«ã‚ãšã€"
        "å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚"
    ),
    "responder_code_2": "# ãƒ¦ãƒ¼ã‚¶ã®è³ªå•:\n{user_input}\n\n# åˆ†æçµæœ:\n{summarizer_answer}",
}


class RouterDecision(BaseModel):
    """Routerã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡ºåŠ›"""

    run_code: bool = Field(
        description="True if the user requests writing AND executing code to produce outputs; False for normal conversation / explanation."
    )
    reason: str = Field(description="Short reason for the decision in Japanese.")


class Plan(BaseModel):
    """Plannerã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡ºåŠ›"""

    steps: List[str] = Field(
        description="Pythonã§å®Ÿè¡Œã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ã‚’çŸ­ã„æ–‡ã§è¡¨ç¾",
        min_items=1,
        max_items=3,
    )


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæœŸåŒ–ãƒ˜ãƒ«ãƒ‘
# =========================
if "agent_prompts" not in st.session_state:
    st.session_state.agent_prompts = DEFAULT_AGENT_PROMPTS.copy()


# =========================
# Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ç”¨æ„
# =========================
def get_web_search_tool():
    return DuckDuckGoSearchRun().invoke


# =========================
# ãƒ˜ãƒ«ãƒ‘: æœ€è¿‘ã®æœ€å°é™ã®ä¼šè©±å±¥æ­´ã‚’ä½œæˆ
# =========================
def get_recent_turn_messages(max_turns: int = 2) -> List[Any]:
    messages: List[Any] = []
    hist = st.session_state.get("history", [])
    if not hist:
        return messages

    recent = hist[-max_turns:]
    for turn in recent:
        u = turn.get("user_input")
        if u:
            messages.append(HumanMessage(content=u))
        a = turn.get("answer")
        if a:
            messages.append(AIMessage(content=a))
    return messages


# ãƒ­ã‚°å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ï¼ˆMessage ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¦‹ã‚„ã™ãã™ã‚‹ï¼‰
def _format_messages_for_log(msgs: List[Any]) -> str:
    out_lines: List[str] = []
    for m in msgs:
        try:
            role = m.__class__.__name__
            content = getattr(m, "content", str(m))
        except Exception:
            role = "Unknown"
            content = str(m)
        out_lines.append(f"- {role}: {content}")
    return "\n".join(out_lines) if out_lines else "(no messages)"


# ãƒ­ã‚°ã‚’çµ±ä¸€ã—ã¦å‡ºã™ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def log_agent_io(agent: str, input_data: Any, output_data: Any):
    try:
        if isinstance(input_data, (list, tuple)):
            input_str = _format_messages_for_log(list(input_data))
        else:
            input_str = str(input_data)
    except Exception:
        input_str = repr(input_data)

    try:
        output_str = str(output_data)
    except Exception:
        output_str = repr(output_data)

    logger.info("=== %s INPUT ===\n%s", agent, input_str)
    logger.info("=== %s OUTPUT ===\n%s", agent, output_str)


# =========================
# LangGraph ã® State å®šç¾©
# =========================
class AppState(TypedDict):
    user_input: str
    mode: Literal["chat", "code"]
    plan: Optional[List[str]]
    executions: Optional[List[str]]
    answer: Optional[str]
    agent_logs: List[Dict[str, Any]]  # UIç”¨ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ­ã‚°


# =========================
# ãƒãƒ¼ãƒ‰å®Ÿè£…ï¼ˆå„æ‰€ã§ session ã® agent_prompts ã‚’å‚ç…§ï¼‰
# =========================
def router_node(state: AppState) -> AppState:
    llm = get_llm("router", temperature=0.0)
    system_content = st.session_state.agent_prompts.get("router", DEFAULT_AGENT_PROMPTS["router"])
    system = SystemMessage(content=system_content)

    recent_msgs = get_recent_turn_messages(max_turns=2)
    messages = [system] + recent_msgs + [HumanMessage(content=state["user_input"])]

    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆinputï¼‰
    log_agent_io("Router", messages, "invoking LLM for decision")

    decision = llm.with_structured_output(RouterDecision).invoke(messages)

    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆoutputï¼‰
    log_agent_io(
        "Router", "(invocation)", f"run_code={decision.run_code} / reason={decision.reason}"
    )

    mode: Literal["chat", "code"] = "code" if decision.run_code else "chat"
    log = {"agent": "Router", "output": f"run_code={decision.run_code} / reason={decision.reason}"}
    return {
        **state,
        "mode": mode,
        "agent_logs": state["agent_logs"] + [log],
    }


class SearchQueries(BaseModel):
    queries: List[str] = Field(min_items=0, max_items=3)


def planner_node(state: AppState) -> AppState:
    llm = get_llm("planner", temperature=0.2)
    system_content = st.session_state.agent_prompts.get("planner", DEFAULT_AGENT_PROMPTS["planner"])
    system = SystemMessage(content=system_content)

    recent_msgs = get_recent_turn_messages(max_turns=1)

    # ãƒ­ã‚°ï¼ˆinput: recent history + user inputï¼‰
    planner_input_msgs = (
        [system] + recent_msgs + [HumanMessage(content=state.get("user_input", ""))]
    )
    log_agent_io("Planner", planner_input_msgs, "invoking LLM to create plan")

    decision: Plan = llm.with_structured_output(Plan).invoke(
        [system] + recent_msgs + [HumanMessage(content=state["user_input"])]
    )

    steps = decision.steps
    log = {"agent": "Planner", "output": "\n".join(f"- {s}" for s in steps)}

    # ãƒ­ã‚°ï¼ˆoutput: stepsï¼‰
    log_agent_io("Planner", "(invocation)", steps)

    return {**state, "plan": steps, "agent_logs": state["agent_logs"] + [log]}


def _extract_code_blocks(text: str) -> List[str]:
    blocks = re.findall(r"```(?:python)?\s*([\s\S]*?)```", text, flags=re.IGNORECASE)
    if blocks:
        return [b.strip() for b in blocks if b.strip()]
    return [text.strip()] if text.strip() else []


def executor_node(
    state: AppState, ui_callback: Optional[Callable[[Dict[str, Any]], None]] = None
) -> AppState:
    llm = get_llm("executor", temperature=0.0)
    executions: List[str] = []
    images_accum: List[str] = []

    tmp_dir = Path("tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)

    for i, step in enumerate(state.get("plan", []) or [], start=1):
        image_path = (tmp_dir / f"step_{i}.png").resolve()
        image_path_str = str(image_path)

        # ãƒ¦ãƒ¼ã‚¶ç·¨é›†ã® executor ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰ã‚’å–å¾—ã—ã€ç”»åƒä¿å­˜ã«é–¢ã™ã‚‹ã€Œå‹•çš„ãƒ«ãƒ¼ãƒ«ã€ã‚’è¿½è¨˜ã™ã‚‹
        user_executor_prompt_base = st.session_state.agent_prompts.get(
            "executor_1", DEFAULT_AGENT_PROMPTS["executor_1"]
        )
        system_content = user_executor_prompt_base.format(image_path_str=image_path_str)

        # ChatPromptTemplate ã‚’ç”¨æ„ï¼ˆsystem + humanï¼‰
        prompt = ChatPromptTemplate.from_messages([("system", system_content), ("human", "{step}")])

        recent_msgs = get_recent_turn_messages(max_turns=1)
        previous_execs_text = "\n\n".join(executions) if executions else ""
        extra_msgs: List[Any] = []
        if previous_execs_text:
            system_content = st.session_state.agent_prompts.get(
                "executor_2", DEFAULT_AGENT_PROMPTS["executor_2"]
            )
            extra_msgs.append(
                HumanMessage(content=system_content.format(previous_execs_text=previous_execs_text))
            )

        prompt_msgs = prompt.format_messages(step=step)
        messages = (
            [SystemMessage(content=prompt_msgs[0].content)]
            if prompt_msgs and isinstance(prompt_msgs[0], SystemMessage)
            else []
        )
        messages = (
            [SystemMessage(content=prompt_msgs[0].content)]
            + recent_msgs
            + extra_msgs
            + [HumanMessage(content=step)]
        )

        # ãƒ­ã‚°ï¼ˆExecutor-step: input: step, recent messages, previous execsï¼‰
        executor_input_summary = {
            "step_index": i,
            "step": step,
            "recent_messages": _format_messages_for_log(recent_msgs),
            "previous_execs": previous_execs_text[:4000],
        }
        log_agent_io(f"Executor-Step-{i}", executor_input_summary, "invoking executor LLM")

        code_text = llm.invoke(messages).content

        # ãƒ­ã‚°ï¼ˆç”Ÿæˆã‚³ãƒ¼ãƒ‰ï¼‰
        log_agent_io(f"Executor-Step-{i}", "(invocation)", code_text[:4000])

        code_blocks = _extract_code_blocks(code_text)

        if not code_blocks:
            exec_report = f"[Step {i}] ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            executions.append(exec_report)
            if ui_callback:
                ui_callback({"agent": f"Executor-Step-{i}", "output": exec_report})
            continue

        code = code_blocks[0]

        # å…ˆé ­ã«å¿…è¦ãªimportæ–‡ã‚’è¿½è¨˜
        code = (
            "import sys\n"
            "import os\n"
            "CURDIR = os.path.dirname(os.path.abspath(__file__))\n"
            "sys.path.append(os.path.join(CURDIR, '..'))\n"
            "from libs import tools\n"
        ) + code

        file_path = tmp_dir / f"step_{i}.py"
        file_path.write_text(code, encoding="utf-8")

        prior_files = set(tmp_dir.iterdir())

        try:
            # 40ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€æ¨™æº–å‡ºåŠ›ãƒ»æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
            result = subprocess.run(
                [sys.executable, str(file_path)],
                capture_output=True,
                text=True,
                timeout=40,
            )
            out = result.stdout.strip() or "(no output)"
            err = result.stderr.strip()
            exec_report_str = f"[Step {i}] OK\n--- file ---\n{file_path}\n--- code ---\n{code}\n--- output ---\n{out}"
            if err:
                exec_report_str += f"\n--- stderr ---\n{err}"
        except Exception as e:
            out = ""
            err = str(e)
            exec_report_str = (
                f"[Step {i}] å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\n--- file ---\n{file_path}\n--- code ---\n{code}"
            )

        # ãƒ­ã‚°ï¼ˆå®Ÿè¡Œçµæœï¼‰
        log_agent_io(f"Executor-Step-{i}", "(execution)", {"stdout": out, "stderr": err})

        images: List[str] = []
        try:
            marker_matches = re.findall(r"IMAGE_SAVED:\s*(\S+)", out)
            for m in marker_matches:
                p = Path(m)
                if not p.is_absolute():
                    p = (tmp_dir / m).resolve()
                if p.exists():
                    images.append(str(p))
                    images_accum.append(str(p))
        except Exception:
            pass

        new_files = set(tmp_dir.iterdir()) - prior_files
        for p in sorted(new_files):
            if p.suffix.lower() in {".png", ".jpg", ".jpeg", ".svg", ".webp", ".gif"}:
                if str(p) not in images:
                    images.append(str(p))
                    images_accum.append(str(p))

        executions.append(exec_report_str)

        step_log = {"agent": f"Executor-Step-{i}", "output": exec_report_str}
        if images:
            step_log["images"] = images

        if ui_callback:
            ui_callback(step_log)

    log = {"agent": "Executor", "output": "\n\n".join(executions)}
    if images_accum:
        log["images"] = images_accum

    # Executor å…¨ä½“ãƒ­ã‚°å‡ºåŠ›
    log_agent_io("Executor", "(all executions)", executions[:4000])

    return {**state, "executions": executions, "agent_logs": state["agent_logs"] + [log]}


def summarizer_node(state: AppState) -> AppState:
    llm = get_llm("summarizer", temperature=0.3)
    joined = "\n\n".join(state.get("executions") or [])

    # ãƒ­ã‚°ï¼ˆinput: å…¨å®Ÿè¡Œãƒ­ã‚°ï¼‰
    log_agent_io("Summarizer", joined[:4000], "invoking summarizer")

    system_content = st.session_state.agent_prompts.get(
        "summarizer_1", DEFAULT_AGENT_PROMPTS["summarizer_1"]
    )
    system = SystemMessage(content=system_content)

    recent_msgs = get_recent_turn_messages(max_turns=1)
    user_input = state.get("user_input", "").strip()
    system_content = st.session_state.agent_prompts.get(
        "summarizer_2", DEFAULT_AGENT_PROMPTS["summarizer_2"]
    )
    messages = (
        [system]
        + recent_msgs
        + [
            HumanMessage(content=joined),
            HumanMessage(content=system_content.format(user_input=user_input)),
        ]
    )

    answer = llm.invoke(messages).content

    # ãƒ­ã‚°ï¼ˆoutput: summaryï¼‰
    log_agent_io("Summarizer", "(invocation)", answer)

    log = {"agent": "Summarizer", "output": answer}
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


def responder_node(state: AppState) -> AppState:
    mode = state.get("mode", "chat")
    generation_llm = get_llm("responder", temperature=0.3)

    if mode == "chat":
        system_content = st.session_state.agent_prompts.get(
            "responder_chat", DEFAULT_AGENT_PROMPTS["responder_chat"]
        )
        system = SystemMessage(content=system_content)
        payload = state.get("user_input", "").strip()
        recent_msgs = get_recent_turn_messages(max_turns=3)

        # ãƒ­ã‚°ï¼ˆinput: recent history + payloadï¼‰
        responder_input = [system] + recent_msgs + [HumanMessage(content=payload)]
        log_agent_io("Responder(chat)", responder_input, "invoking responder LLM")
    else:
        system_content = st.session_state.agent_prompts.get(
            "responder_code_1", DEFAULT_AGENT_PROMPTS["responder_code_1"]
        )
        system = SystemMessage(content=(system_content))

        user_input = state.get("user_input", "").strip()
        summarizer_answer = state.get("answer", "").strip()
        system_content = st.session_state.agent_prompts.get(
            "responder_code_2", DEFAULT_AGENT_PROMPTS["responder_code_2"]
        )
        payload = system_content.format(user_input=user_input, summarizer_answer=summarizer_answer)
        recent_msgs = get_recent_turn_messages(max_turns=1)

        # ãƒ­ã‚°ï¼ˆinput: user_input + summarizerï¼‰
        responder_input = [system] + recent_msgs + [HumanMessage(content=payload or "")]
        log_agent_io("Responder(code)", responder_input, "invoking responder LLM")

    messages = [system] + recent_msgs + [HumanMessage(content=payload or "")]
    final_answer = generation_llm.invoke(messages).content

    # ãƒ­ã‚°ï¼ˆoutput: final answerï¼‰
    log_agent_io("Responder", "(invocation)", final_answer)

    log = {"agent": f"Responder(mode={mode})", "output": final_answer}
    return {**state, "answer": final_answer, "agent_logs": state["agent_logs"] + [log]}


# =========================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# =========================
def build_graph():
    g = StateGraph(AppState)

    g.add_node("router", router_node)
    g.add_node("planner", planner_node)
    g.add_node("executor", executor_node)
    g.add_node("summarizer", summarizer_node)
    g.add_node("responder", responder_node)

    g.set_entry_point("router")

    def route_decision(state: AppState):
        return state["mode"]

    g.add_conditional_edges(
        "router",
        route_decision,
        {
            "chat": "responder",
            "code": "planner",
        },
    )

    g.add_edge("planner", "executor")
    g.add_edge("executor", "summarizer")
    g.add_edge("summarizer", "responder")
    g.add_edge("responder", END)

    return g.compile()


GRAPH = build_graph()


# =========================
# Streamlit UI (ã‚¿ãƒ–åŒ–)
# =========================
st.set_page_config(page_title="LLM Multi-Agent Demo", page_icon="ğŸ¤–", layout="centered")
tabs = st.tabs(["ã‚¢ãƒ—ãƒª", "è©³ç´°è¨­å®š"])
app_tab, settings_tab = tabs[0], tabs[1]

with app_tab:
    st.title("ğŸ¤– LLM Multi-Agent Demo (LangGraph + LangChain + Streamlit)")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šãƒ¡ãƒ¢ãƒª
    if "history" not in st.session_state:
        st.session_state.history = []
    if "thread_memory" not in st.session_state:
        st.session_state.thread_memory = []

    # UI ãƒ˜ãƒ«ãƒ‘: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤ºã™ã‚‹
    def display_agent_log(container: st.delta_generator.DeltaGenerator, log: Dict[str, Any]):
        with container.expander(f"ğŸ§© {log.get('agent')}"):
            out = log.get("output")
            notes = log.get("notes")
            if isinstance(out, (dict, list)):
                st.json(out)
            else:
                st.write(out)
            if notes:
                st.caption("notes:")
                st.json(notes)

    def run_graph_stepwise(
        init_state: AppState, ui_container: st.delta_generator.DeltaGenerator
    ) -> AppState:
        state = init_state

        # Router
        state = router_node(state)
        display_agent_log(ui_container, state["agent_logs"][-1])

        if state["mode"] == "chat":
            state = responder_node(state)
            display_agent_log(ui_container, state["agent_logs"][-1])
            if state.get("answer"):
                ui_container.success(state["answer"])
            return state

        # Code mode
        state = planner_node(state)
        display_agent_log(ui_container, state["agent_logs"][-1])

        def executor_ui_callback(log: Dict[str, Any]):
            display_agent_log(ui_container, log)

        state = executor_node(state, ui_callback=executor_ui_callback)
        # Executor å…¨ä½“ãƒ­ã‚°è¡¨ç¤º
        display_agent_log(ui_container, state["agent_logs"][-1])

        state = summarizer_node(state)
        display_agent_log(ui_container, state["agent_logs"][-1])

        state = responder_node(state)
        display_agent_log(ui_container, state["agent_logs"][-1])

        executor_images: List[str] = []
        for log in state.get("agent_logs", []):
            imgs = log.get("images")
            if imgs:
                executor_images.extend(imgs)

        if state.get("answer"):
            ui_container.success(state["answer"])

        if executor_images:
            try:
                ui_container.image(executor_images, use_column_width=True)
            except Exception as e:
                ui_container.warning(f"å›³ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        return state

    with st.form(key="user_form", clear_on_submit=False):
        user_input = st.text_area(
            "æŒ‡ç¤ºæ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            height=160,
            placeholder="ä¾‹ï¼šè£½å“â—‹â—‹ã€è©•ä¾¡å€¤â–³â–³ã®ç®¡ç†å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦\nä¾‹ï¼šè£½å“â—‹â—‹ã€è©•ä¾¡å€¤â–³â–³ãŒç•°å¸¸ã‹åˆ¤å®šã—ã¦",
        )
        submitted = st.form_submit_button("é€ä¿¡")

    if submitted and user_input.strip():
        st.session_state.thread_memory.append(user_input.strip())

        init_state: AppState = {
            "user_input": user_input.strip(),
            "mode": "chat",
            "plan": None,
            "executions": None,
            "answer": None,
            "agent_logs": [],
        }

        ui_container = st.container()
        result_state: AppState = run_graph_stepwise(init_state, ui_container)
        st.session_state.history.append(result_state)

    # å±¥æ­´è¡¨ç¤º
    if st.session_state.history:
        st.subheader("ğŸ“š ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´")
        for idx, turn in enumerate(reversed(st.session_state.history), start=1):
            st.markdown(f"### Turn {len(st.session_state.history) - idx + 1}")
            for log in turn.get("agent_logs", []):
                with st.expander(f"ğŸ§© {log.get('agent')}"):
                    st.write(log.get("output"))
                    if "notes" in log and log["notes"]:
                        st.caption("notes:")
                        st.json(log["notes"])
            if turn.get("answer"):
                st.success(turn["answer"])
    else:
        st.info("æœ€åˆã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    st.caption(
        "Tips: ãƒ«ãƒ¼ã‚¿ãŒã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã¨åˆ¤æ–­ã™ã‚‹ã¨ã€Planner â†’ Executor â†’ Summarizer ã®é †ã§å‹•ãã¾ã™ã€‚"
        "é€šå¸¸ä¼šè©±ã¨åˆ¤æ–­ã™ã‚‹ã¨ã€ç›´æ¥ Responder ãŒæ¤œç´¢ï¼ˆå¿…è¦æ™‚ï¼‰ã‚’è¡Œã£ã¦å¿œç­”ã—ã¾ã™ã€‚"
    )

with settings_tab:
    st.header("ğŸ›  è©³ç´°è¨­å®š â€” å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® system ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç·¨é›†")
    st.write(
        "ã“ã“ã§ç·¨é›†ã—ãŸ system ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«ä¿å­˜ã•ã‚Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œæ™‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
    )
    st.write(
        "â€» Executor ã®å ´åˆã€å›³ã®ä¿å­˜ã‚„ `IMAGE_SAVED:` ãƒãƒ¼ã‚«ãƒ¼ç­‰ã®ç”»åƒãƒ«ãƒ¼ãƒ«ã¯è‡ªå‹•çš„ã«è¿½è¨˜ã•ã‚Œã¾ã™ã€‚"
    )

    cols = st.columns([1, 1])
    with cols[0]:
        if st.button("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™"):
            st.session_state.agent_prompts = DEFAULT_AGENT_PROMPTS.copy()
            st.rerun()

    # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç·¨é›†ã‚¨ãƒªã‚¢
    for agent in DEFAULT_AGENT_PROMPTS.keys():
        label = f"{agent}"
        current = st.session_state.agent_prompts.get(agent, DEFAULT_AGENT_PROMPTS[agent])
        new_text = st.text_area(label, value=current, height=160, key=f"prompt_{agent}")
        # å¤‰æ›´ãŒã‚ã‚Œã°ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åæ˜ 
        st.session_state.agent_prompts[agent] = new_text

    st.caption("ç·¨é›†å¾Œã¯ã‚¢ãƒ—ãƒªã‚¿ãƒ–ã«æˆ»ã‚Šã€é€šå¸¸ã©ãŠã‚Šé€ä¿¡ã—ã¦ãã ã•ã„ã€‚")
