import contextlib
import io
import os
import re
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, TypedDict

import streamlit as st
from dotenv import load_dotenv

# ãƒ„ãƒ¼ãƒ«ï¼ˆWebæ¤œç´¢ & Pythonå®Ÿè¡Œï¼‰
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureChatOpenAI, ChatOpenAI

# LangGraph
from langgraph.graph import END, StateGraph

# --- LangChain / OpenAI ---
from pydantic import BaseModel, Field

# SerpAPI ã‚’ä½¿ã„ãŸã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼ˆ.envã«SERPAPI_API_KEYãŒå¿…è¦ï¼‰
# from langchain_community.tools import SerpAPIWrapper


# =========================
# Env èª­ã¿è¾¼ã¿ & LLM Factory
# =========================
load_dotenv()


# å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«/ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®šã‚’.envã‹ã‚‰èª­ã¿è¾¼ã¿
AGENT_CONFIG = {
    "router": {
        "model": os.getenv("ROUTER_MODEL"),
        "azure_deployment": os.getenv("ROUTER_AZURE_DEPLOYMENT"),
    },
    "chat": {
        "model": os.getenv("CHAT_MODEL"),
        "azure_deployment": os.getenv("CHAT_AZURE_DEPLOYMENT"),
    },
    "planner": {
        "model": os.getenv("PLANNER_MODEL"),
        "azure_deployment": os.getenv("PLANNER_AZURE_DEPLOYMENT"),
    },
    "executor": {
        "model": os.getenv("EXECUTOR_MODEL"),
        "azure_deployment": os.getenv("EXECUTOR_AZURE_DEPLOYMENT"),
    },
    "summarizer": {
        "model": os.getenv("SUMMARIZER_MODEL"),
        "azure_deployment": os.getenv("SUMMARIZER_AZURE_DEPLOYMENT"),
    },
}


def get_llm(agent: str, temperature: float = 0.2):
    """
    agentåã«å¿œã˜ãŸ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™ã€‚
    - Azure ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚Œã° Azure ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤åã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«åˆ‡æ›¿ï¼‰
    - ãã‚Œä»¥å¤–ã¯ OpenAI API ã‚’ä½¿ç”¨
    """
    cfg = AGENT_CONFIG[agent]
    model = cfg["model"]
    azure_deployment = cfg["azure_deployment"]

    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")

    if azure_endpoint and azure_key and azure_deployment:
        return AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_key,
            api_version=azure_api_version,
            deployment_name=azure_deployment,
            temperature=temperature,
        )

    # é€šå¸¸ã® OpenAI API
    return ChatOpenAI(model=model, temperature=temperature)


# =========================
# Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ç”¨æ„
# =========================
def get_web_search_tool():
    # SERPAPI_API_KEY ãŒã‚ã‚Œã° SerpAPI ã‚’ä½¿ã£ã¦ã‚‚ã‚ˆã„ãŒã€ã“ã“ã§ã¯ DuckDuckGo ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¡ç”¨
    # serp_key = os.getenv("SERPAPI_API_KEY")
    # if serp_key:
    #     serp = SerpAPIWrapper(serpapi_api_key=serp_key)
    #     return serp.run
    return DuckDuckGoSearchRun().invoke


# =========================
# ãƒ«ãƒ¼ã‚¿å‡ºåŠ› (Structured)
# =========================
class RouterDecision(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶æŒ‡ç¤ºãŒ 'ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™' ç¨®ã‹ã©ã†ã‹ã®åˆ†é¡"""

    run_code: bool = Field(
        description="True if the user requests writing AND executing code to produce outputs; False for normal conversation / explanation."
    )
    reason: str = Field(description="Short reason for the decision in Japanese.")


# =========================
# LangGraph ã® State å®šç¾©
# =========================
class AppState(TypedDict):
    user_input: str
    mode: Literal["chat", "code"]
    plan: Optional[List[str]]
    executions: Optional[List[str]]
    answer: Optional[str]
    agent_logs: List[Dict[str, Any]]  # UIç”¨ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ­ã‚°


# =========================
# ãƒãƒ¼ãƒ‰å®Ÿè£…
# =========================


def router_node(state: AppState) -> AppState:
    """ãƒ¦ãƒ¼ã‚¶æŒ‡ç¤ºãŒé€šå¸¸ä¼šè©±ã‹ã€ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒå¿…è¦ã‹ã‚’åˆ¤å®š"""
    llm = get_llm("router", temperature=0.0)
    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯å…¥åŠ›ãŒã€é€šå¸¸ã®ä¼šè©±ã‹/ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ã¹ãã‹ã€ã‚’åˆ¤å®šã™ã‚‹åˆ†é¡å™¨ã§ã™ã€‚"
            "ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ»è¨ˆç®—ãƒ»ã‚°ãƒ©ãƒ•ä½œæˆãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€æ˜ã‚‰ã‹ã«å®Ÿè¡ŒçµæœãŒå¿…è¦ãªå ´åˆã¯ run_code=Trueã€‚"
            "ä¸€èˆ¬çš„ãªQAã€è€ƒå¯Ÿã€è¦ç´„ã€ã‚¬ã‚¤ãƒ‰ã€Webæ¤œç´¢ã§ç°¡æ½”ã™ã‚‹ã‚‚ã®ã¯ run_code=False ã¨ã™ã‚‹ã€‚"
        )
    )
    human = HumanMessage(content=state["user_input"])
    decision = llm.with_structured_output(RouterDecision).invoke([system, human])
    mode: Literal["chat", "code"] = "code" if decision.run_code else "chat"
    log = {"agent": "Router", "output": f"run_code={decision.run_code} / reason={decision.reason}"}
    return {
        **state,
        "mode": mode,
        "agent_logs": state["agent_logs"] + [log],
    }


class SearchQueries(BaseModel):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªå€™è£œã€‚æ¤œç´¢ä¸è¦ãªå ´åˆã¯ç©ºãƒªã‚¹ãƒˆã«ã™ã‚‹ã€‚"""

    queries: List[str] = Field(
        description="DuckDuckGoã§æ¤œç´¢ã™ã‚‹çŸ­ã„ã‚¯ã‚¨ãƒªã€‚æ¤œç´¢ä¸è¦ãªã‚‰ç©ºãƒªã‚¹ãƒˆã€‚",
        min_items=0,
        max_items=3,
    )


def chat_agent_node(state: AppState) -> AppState:
    """é€šå¸¸ä¼šè©±ï¼‹Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    llm = get_llm("chat", temperature=0.3)
    search = get_web_search_tool()

    # æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆstructured outputï¼‰
    planner_llm = get_llm("chat", temperature=0.0)
    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒãƒ£ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«å¿…è¦ãªã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è€ƒãˆã¾ã™ã€‚"
            "DuckDuckGoã§æœ‰åŠ¹ãªçŸ­ã„æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
            "æ¤œç´¢ä¸è¦ã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
        )
    )
    human = HumanMessage(content=state["user_input"])
    decision: SearchQueries = planner_llm.with_structured_output(SearchQueries).invoke(
        [system, human]
    )

    queries = decision.queries
    results: List[str] = []

    if queries:
        for q in queries:
            try:
                res = search(q)
                results.append(f"[{q}]\n{res}")
            except Exception as e:
                results.append(f"[{q}] æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

    # æœ€çµ‚å›ç­”
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ã‚ãªãŸã¯æœ‰èƒ½ãªãƒªã‚µãƒ¼ãƒãƒ£ã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœï¼ˆç©ºã®ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼‰ã‚’å‚è€ƒã«ã€"
                "æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰ç®‡æ¡æ›¸ãã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚",
            ),
            ("human", "{q}"),
            ("ai", "{snippets}"),
        ]
    )
    answer = llm.invoke(
        prompt.format_messages(
            q=state["user_input"],
            snippets="\n\n".join(results) if results else "ï¼ˆæ¤œç´¢ãªã—ï¼‰",
        )
    ).content

    log = {
        "agent": "ChatAgent(with WebSearch)",
        "output": answer,
        "notes": {"queries": queries, "search_snippets": results},
    }
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


class Plan(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶è¦æ±‚ã‚’æº€ãŸã™ãŸã‚ã®å…·ä½“çš„ãªå‡¦ç†æ‰‹é †"""

    steps: List[str] = Field(
        description="Pythonã§å®Ÿè¡Œã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’çŸ­ã„æ–‡ã§è¡¨ç¾",
        min_items=1,
        max_items=3,
    )


def planner_node(state: AppState) -> AppState:
    """ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒå¿…è¦ãªå ´åˆã®è¨ˆç”»ç«‹æ¡ˆï¼ˆstructured outputç‰ˆï¼‰"""
    llm = get_llm("planner", temperature=0.2)

    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶è¦æ±‚ã‚’æº€ãŸã™ãŸã‚ã®å®Ÿè¡Œè¨ˆç”»ã‚’ç«‹ã¦ã¾ã™ã€‚"
            "è¨ˆç”»ã¯å¿…ãš Python ã§å®Ÿè¡Œå¯èƒ½ãªå…·ä½“çš„ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€"
            "çŸ­ã„æ–‡ã®ãƒªã‚¹ãƒˆã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"
            "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ã‚¡ã‚¤ãƒ«æ›¸è¾¼ã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚"
        )
    )
    human = HumanMessage(content=state["user_input"])

    # structured_outputã‚’ä½¿ã£ã¦ steps ã‚’ç›´æ¥æŠ½å‡º
    decision: Plan = llm.with_structured_output(Plan).invoke([system, human])

    steps = decision.steps
    log = {"agent": "Planner", "output": "\n".join(f"- {s}" for s in steps)}

    return {**state, "plan": steps, "agent_logs": state["agent_logs"] + [log]}


def _extract_code_blocks(text: str) -> List[str]:
    """```python ... ``` ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºã€‚ãªã‘ã‚Œã°å…¨æ–‡ã‚’1ãƒ–ãƒ­ãƒƒã‚¯æ‰±ã„"""
    blocks = re.findall(r"```(?:python)?\s*([\s\S]*?)```", text, flags=re.IGNORECASE)
    if blocks:
        return [b.strip() for b in blocks if b.strip()]
    return [text.strip()] if text.strip() else []


def executor_node(state: AppState) -> AppState:
    """å®Ÿè¡Œè€…ï¼šå„ã‚¹ãƒ†ãƒƒãƒ—ã‚’Pythonã‚³ãƒ¼ãƒ‰åŒ–ã—ã€/tmp/ ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã£ã¦å®Ÿè¡Œ"""
    llm = get_llm("executor", temperature=0.0)
    executions: List[str] = []

    tmp_dir = Path("tmp")

    for i, step in enumerate(state.get("plan", []) or [], start=1):
        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’ Python ã‚³ãƒ¼ãƒ‰åŒ–
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ã‚ãªãŸã¯Pythonã‚³ãƒ¼ãƒ€ã§ã™ã€‚ä»¥ä¸‹ã®ã€å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã€ã‚’æº€ãŸã™æœ€å°ã®å®Ÿè¡Œå¯èƒ½ãªPythonã‚³ãƒ¼ãƒ‰ã‚’"
                    "1ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è¡¨ç¤ºã¯ print ã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚"
                    "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã¯ã—ãªã„ã§ãã ã•ã„ã€‚",
                ),
                ("human", "{step}"),
            ]
        )
        code_text = llm.invoke(prompt.format_messages(step=step)).content
        code_blocks = _extract_code_blocks(code_text)

        if not code_blocks:
            executions.append(f"[Step {i}] ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            continue

        code = code_blocks[0]

        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãå‡ºã—
        file_path = tmp_dir / f"step_{i}.py"
        file_path.write_text(code, encoding="utf-8")

        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ
        try:
            result = subprocess.run(
                [sys.executable, str(file_path)],
                capture_output=True,
                text=True,
                timeout=20,
            )
            out = result.stdout.strip() or "(no output)"
            err = result.stderr.strip()
            exec_report = f"[Step {i}] OK\n--- file ---\n{file_path}\n--- code ---\n{code}\n--- output ---\n{out}"
            if err:
                exec_report += f"\n--- stderr ---\n{err}"
        except Exception as e:
            exec_report = (
                f"[Step {i}] å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\n--- file ---\n{file_path}\n--- code ---\n{code}"
            )

        executions.append(exec_report)

    log = {"agent": "Executor", "output": "\n\n".join(executions)}
    return {**state, "executions": executions, "agent_logs": state["agent_logs"] + [log]}


def summarizer_node(state: AppState) -> AppState:
    """å®Ÿè¡Œçµæœã®è¦ç´„"""
    llm = get_llm("summarizer", temperature=0.3)
    joined = "\n\n".join(state.get("executions") or [])
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ã‚ãªãŸã¯ã‚µãƒãƒ©ã‚¤ã‚¶ã§ã™ã€‚ä»¥ä¸‹ã®å®Ÿè¡Œãƒ­ã‚°ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ã®è¦æ±‚ã«å¯¾ã™ã‚‹çµæœã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                "å¿…è¦ãªã‚‰å®Ÿè¡Œå€¤ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚",
            ),
            ("human", "{log}"),
        ]
    )
    answer = llm.invoke(prompt.format_messages(log=joined)).content
    log = {"agent": "Summarizer", "output": answer}
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


def responder_node(state: AppState) -> AppState:
    """Summarizer ã®è¦ç´„ã‚’ã‚‚ã¨ã«ã€ãƒ¦ãƒ¼ã‚¶å‘ã‘ã®æœ€çµ‚å›ç­”ã‚’è‡ªç„¶ãªæ–‡ç« ã«æ•´å½¢"""
    llm = get_llm("summarizer", temperature=0.5)  # summarizerã¨åŒã˜ã§ã‚‚è‰¯ã„ãŒåˆ‡æ›¿å¯
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ã‚ãªãŸã¯å›ç­”è€…ã§ã™ã€‚ä»¥ä¸‹ã®è¦ç´„ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ã«ã‚ã‹ã‚Šã‚„ã™ãè‡ªç„¶ãªæ—¥æœ¬èªã§æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                "ä½™è¨ˆãªãƒ¡ã‚¿æƒ…å ±ï¼ˆå®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ãªã©ï¼‰ã¯å«ã‚ãšã€å¿…è¦ãªéƒ¨åˆ†ã ã‘ç°¡æ½”ã«ä¼ãˆã¦ãã ã•ã„ã€‚",
            ),
            ("human", "{summary}"),
        ]
    )
    answer = llm.invoke(prompt.format_messages(summary=state.get("answer", ""))).content
    log = {"agent": "Responder", "output": answer}
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


# =========================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# =========================
def build_graph():
    g = StateGraph(AppState)

    g.add_node("router", router_node)
    g.add_node("chat_agent", chat_agent_node)
    g.add_node("planner", planner_node)
    g.add_node("executor", executor_node)
    g.add_node("summarizer", summarizer_node)
    g.add_node("responder", responder_node)

    g.set_entry_point("router")

    def route_decision(state: AppState):
        return state["mode"]

    g.add_conditional_edges(
        "router",
        route_decision,
        {
            "chat": "chat_agent",
            "code": "planner",
        },
    )

    g.add_edge("chat_agent", END)
    g.add_edge("planner", "executor")
    g.add_edge("executor", "summarizer")
    g.add_edge("summarizer", "responder")
    g.add_edge("responder", END)

    return g.compile()


GRAPH = build_graph()


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="LLM Multi-Agent Demo", page_icon="ğŸ¤–", layout="centered")
st.title("ğŸ¤– LLM Multi-Agent Demo (LangGraph + LangChain + Streamlit)")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šãƒ¡ãƒ¢ãƒª
if "history" not in st.session_state:
    st.session_state.history = []  # å„ã‚¿ãƒ¼ãƒ³ã® state ã‚’ä¿å­˜
if "thread_memory" not in st.session_state:
    # ãƒ¦ãƒ¼ã‚¶ç™ºè©±ã‚’å˜ç´”ã«é€£çµã§æŒã¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦é«˜åº¦ãªãƒ¡ãƒ¢ãƒªã«ç½®æ›å¯ï¼‰
    st.session_state.thread_memory = []

with st.form(key="user_form", clear_on_submit=False):
    user_input = st.text_area(
        "æŒ‡ç¤ºæ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        height=160,
        placeholder="ä¾‹ï¼š2020ã€œ2024ã®ä¸–ç•ŒGDPä¸Šä½5ã‹å›½ã‚’ä¸€è¦§ã«ã—ã¦è€ƒå¯Ÿã—ã¦\nä¾‹ï¼šÏ€ã‚’100æ¡è¨ˆç®—ã—ã¦å…ˆé ­20æ¡ã ã‘è¡¨ç¤ºã—ã¦",
    )
    submitted = st.form_submit_button("é€ä¿¡")

if submitted and user_input.strip():
    # ã‚¹ãƒ¬ãƒƒãƒ‰è¨˜æ†¶ã‚’ä»˜åŠ ï¼ˆæœ¬ãƒ‡ãƒ¢ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã« Human ã®å±¥æ­´ã‚’æ¸¡ã™ã ã‘ï¼‰
    st.session_state.thread_memory.append(user_input.strip())

    init_state: AppState = {
        "user_input": user_input.strip(),
        "mode": "chat",
        "plan": None,
        "executions": None,
        "answer": None,
        "agent_logs": [],
    }

    # LangGraph å®Ÿè¡Œ
    result_state: AppState = GRAPH.invoke(init_state)

    # å±¥æ­´ã«ä¿å­˜ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜æ†¶ä¿æŒï¼‰
    st.session_state.history.append(result_state)

# å±¥æ­´ã®è¡¨ç¤º
if st.session_state.history:
    st.subheader("ğŸ“š ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´")
    for idx, turn in enumerate(reversed(st.session_state.history), start=1):
        st.markdown(f"### Turn {len(st.session_state.history) - idx + 1}")
        for log in turn.get("agent_logs", []):
            with st.expander(f"ğŸ§© {log.get('agent')}"):
                st.write(log.get("output"))
                if "notes" in log and log["notes"]:
                    st.caption("notes:")
                    st.json(log["notes"])
        if turn.get("answer"):
            st.success(turn["answer"])
else:
    st.info("æœ€åˆã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


st.caption(
    "Tips: ãƒ«ãƒ¼ã‚¿ãŒã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã¨åˆ¤æ–­ã™ã‚‹ã¨ã€Planner â†’ Executor â†’ Summarizer ã®é †ã§å‹•ãã¾ã™ã€‚"
    "é€šå¸¸ä¼šè©±ã¨åˆ¤æ–­ã™ã‚‹ã¨ã€Webæ¤œç´¢ä»˜ãã® ChatAgent ãŒå¿œç­”ã—ã¾ã™ã€‚"
)
