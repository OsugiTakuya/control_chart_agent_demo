import contextlib
import io
import os
import re
import subprocess
import sys
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, TypedDict

import streamlit as st
from dotenv import load_dotenv

# ãƒ„ãƒ¼ãƒ«ï¼ˆWebæ¤œç´¢ & Pythonå®Ÿè¡Œï¼‰
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureChatOpenAI, ChatOpenAI

# LangGraph
from langgraph.graph import END, StateGraph

# --- LangChain / OpenAI ---
from pydantic import BaseModel, Field

# SerpAPI ã‚’ä½¿ã„ãŸã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼ˆ.envã«SERPAPI_API_KEYãŒå¿…è¦ï¼‰
# from langchain_community.tools import SerpAPIWrapper


# =========================
# Env èª­ã¿è¾¼ã¿ & LLM Factory
# =========================
load_dotenv()


# å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«/ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®šã‚’.envã‹ã‚‰èª­ã¿è¾¼ã¿
AGENT_CONFIG = {
    "router": {
        "model": os.getenv("ROUTER_MODEL"),
        "azure_deployment": os.getenv("ROUTER_AZURE_DEPLOYMENT"),
    },
    "chat": {
        "model": os.getenv("CHAT_MODEL"),
        "azure_deployment": os.getenv("CHAT_AZURE_DEPLOYMENT"),
    },
    "planner": {
        "model": os.getenv("PLANNER_MODEL"),
        "azure_deployment": os.getenv("PLANNER_AZURE_DEPLOYMENT"),
    },
    "executor": {
        "model": os.getenv("EXECUTOR_MODEL"),
        "azure_deployment": os.getenv("EXECUTOR_AZURE_DEPLOYMENT"),
    },
    "summarizer": {
        "model": os.getenv("SUMMARIZER_MODEL"),
        "azure_deployment": os.getenv("SUMMARIZER_AZURE_DEPLOYMENT"),
    },
}


def get_llm(agent: str, temperature: float = 0.2):
    """
    agentåã«å¿œã˜ãŸ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™ã€‚
    - Azure ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚Œã° Azure ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤åã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«åˆ‡æ›¿ï¼‰
    - ãã‚Œä»¥å¤–ã¯ OpenAI API ã‚’ä½¿ç”¨
    """
    cfg = AGENT_CONFIG[agent]
    model = cfg["model"]
    azure_deployment = cfg["azure_deployment"]

    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")

    if azure_endpoint and azure_key and azure_deployment:
        return AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_key,
            api_version=azure_api_version,
            deployment_name=azure_deployment,
            temperature=temperature,
        )

    # é€šå¸¸ã® OpenAI API
    return ChatOpenAI(model=model, temperature=temperature)


# =========================
# Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ç”¨æ„
# =========================
def get_web_search_tool():
    # SERPAPI_API_KEY ãŒã‚ã‚Œã° SerpAPI ã‚’ä½¿ã£ã¦ã‚‚ã‚ˆã„ãŒã€ã“ã“ã§ã¯ DuckDuckGo ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¡ç”¨
    # serp_key = os.getenv("SERPAPI_API_KEY")
    # if serp_key:
    #     serp = SerpAPIWrapper(serpapi_api_key=serp_key)
    #     return serp.run
    return DuckDuckGoSearchRun().invoke


# =========================
# ãƒ˜ãƒ«ãƒ‘: æœ€è¿‘ã®æœ€å°é™ã®ä¼šè©±å±¥æ­´ã‚’ä½œæˆ
# =========================
def get_recent_turn_messages(max_turns: int = 2) -> List[Any]:
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´(st.session_state.history)ã‹ã‚‰ç›´è¿‘ã® user/assistant ã®ã‚„ã‚Šå–ã‚Šã‚’
    HumanMessage / AIMessage ã®ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚å„ invoke ã«æ¸¡ã™æœ€å°é™ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã€‚
    """
    messages: List[Any] = []
    hist = st.session_state.get("history", [])
    if not hist:
        return messages

    recent = hist[-max_turns:]
    for turn in recent:
        u = turn.get("user_input")
        if u:
            messages.append(HumanMessage(content=u))
        a = turn.get("answer")
        if a:
            messages.append(AIMessage(content=a))
    return messages


# =========================
# ãƒ«ãƒ¼ã‚¿å‡ºåŠ› (Structured)
# =========================
class RouterDecision(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶æŒ‡ç¤ºãŒ 'ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™' ç¨®ã‹ã©ã†ã‹ã®åˆ†é¡"""

    run_code: bool = Field(
        description="True if the user requests writing AND executing code to produce outputs; False for normal conversation / explanation."
    )
    reason: str = Field(description="Short reason for the decision in Japanese.")


# =========================
# LangGraph ã® State å®šç¾©
# =========================
class AppState(TypedDict):
    user_input: str
    mode: Literal["chat", "code"]
    plan: Optional[List[str]]
    executions: Optional[List[str]]
    answer: Optional[str]
    agent_logs: List[Dict[str, Any]]  # UIç”¨ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ­ã‚°


# =========================
# ãƒãƒ¼ãƒ‰å®Ÿè£…
# =========================


def router_node(state: AppState) -> AppState:
    """ãƒ¦ãƒ¼ã‚¶æŒ‡ç¤ºãŒé€šå¸¸ä¼šè©±ã‹ã€ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒå¿…è¦ã‹ã‚’åˆ¤å®š"""
    llm = get_llm("router", temperature=0.0)
    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯å…¥åŠ›ãŒã€é€šå¸¸ã®ä¼šè©±ã‹/ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ã¹ãã‹ã€ã‚’åˆ¤å®šã™ã‚‹åˆ†é¡å™¨ã§ã™ã€‚"
            "ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ»è¨ˆç®—ãƒ»ã‚°ãƒ©ãƒ•ä½œæˆãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€æ˜ã‚‰ã‹ã«å®Ÿè¡ŒçµæœãŒå¿…è¦ãªå ´åˆã¯ run_code=Trueã€‚"
            "ä¸€èˆ¬çš„ãªQAã€è€ƒå¯Ÿã€è¦ç´„ã€ã‚¬ã‚¤ãƒ‰ã€Webæ¤œç´¢ã§ç°¡æ½”ã™ã‚‹ã‚‚ã®ã¯ run_code=False ã¨ã™ã‚‹ã€‚"
        )
    )

    # æœ€å°é™ã®å±¥æ­´ï¼ˆç›´è¿‘ã® turn ã‚’æœ€å¤§2ã¤ã¾ã§ï¼‰ã‚’æ¸¡ã™
    recent_msgs = get_recent_turn_messages(max_turns=2)
    messages = [system] + recent_msgs + [HumanMessage(content=state["user_input"])]

    decision = llm.with_structured_output(RouterDecision).invoke(messages)
    mode: Literal["chat", "code"] = "code" if decision.run_code else "chat"
    log = {"agent": "Router", "output": f"run_code={decision.run_code} / reason={decision.reason}"}
    return {
        **state,
        "mode": mode,
        "agent_logs": state["agent_logs"] + [log],
    }


class SearchQueries(BaseModel):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªå€™è£œã€‚æ¤œç´¢ä¸è¦ãªå ´åˆã¯ç©ºãƒªã‚¹ãƒˆã«ã™ã‚‹ã€‚"""

    queries: List[str] = Field(
        description="DuckDuckGoã§æ¤œç´¢ã™ã‚‹çŸ­ã„ã‚¯ã‚¨ãƒªã€‚æ¤œç´¢ä¸è¦ãªã‚‰ç©ºãƒªã‚¹ãƒˆã€‚",
        min_items=0,
        max_items=3,
    )


def chat_agent_node(state: AppState) -> AppState:
    """é€šå¸¸ä¼šè©±ï¼‹Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    llm = get_llm("chat", temperature=0.3)
    search = get_web_search_tool()

    # æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆstructured outputï¼‰
    planner_llm = get_llm("chat", temperature=0.0)
    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒãƒ£ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«å¿…è¦ãªã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è€ƒãˆã¾ã™ã€‚"
            "DuckDuckGoã§æœ‰åŠ¹ãªçŸ­ã„æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
            "æ¤œç´¢ä¸è¦ã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
        )
    )

    # å±¥æ­´ã¯æœ€è¿‘ã®ä¼šè©±ã®ã¿ã‚’æ¸¡ã™ï¼ˆéå»2ã‚¿ãƒ¼ãƒ³ã¾ã§ï¼‰
    recent_msgs = get_recent_turn_messages(max_turns=2)
    decision: SearchQueries = planner_llm.with_structured_output(SearchQueries).invoke(
        [system] + recent_msgs + [HumanMessage(content=state["user_input"])]
    )

    queries = decision.queries
    results: List[str] = []

    if queries:
        for q in queries:
            try:
                res = search(q)
                results.append(f"[{q}]\n{res}")
            except Exception as e:
                results.append(f"[{q}] æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

    # æœ€çµ‚å›ç­”: ç›´è¿‘ã®ä¼šè©±ï¼ˆæœ€å¤§3ã‚¿ãƒ¼ãƒ³ï¼‰ï¼‹æ¤œç´¢çµæœã‚’æ¸¡ã™ï¼ˆå¿…è¦æœ€å°é™ï¼‰
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ã‚ãªãŸã¯æœ‰èƒ½ãªãƒªã‚µãƒ¼ãƒãƒ£ã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœï¼ˆç©ºã®ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼‰ã‚’å‚è€ƒã«ã€"
                "æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰ç®‡æ¡æ›¸ãã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚",
            ),
            ("human", "{q}"),
            ("ai", "{snippets}"),
        ]
    )

    base_msgs = get_recent_turn_messages(max_turns=3)
    prompt_msgs = prompt.format_messages(
        q=state["user_input"],
        snippets="\n\n".join(results) if results else "ï¼ˆæ¤œç´¢ãªã—ï¼‰",
    )

    # recent_msgs ã‚’å‰ã«ä»˜ã‘ã¦ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¤ã¤å†—é•·ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
    answer_msgs = base_msgs + prompt_msgs
    answer = llm.invoke(answer_msgs).content

    log = {
        "agent": "ChatAgent(with WebSearch)",
        "output": answer,
        "notes": {"queries": queries, "search_snippets": results},
    }
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


class Plan(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶è¦æ±‚ã‚’æº€ãŸã™ãŸã‚ã®å…·ä½“çš„ãªå‡¦ç†æ‰‹é †"""

    steps: List[str] = Field(
        description="Pythonã§å®Ÿè¡Œã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’çŸ­ã„æ–‡ã§è¡¨ç¾",
        min_items=1,
        max_items=3,
    )


def planner_node(state: AppState) -> AppState:
    """ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒå¿…è¦ãªå ´åˆã®è¨ˆç”»ç«‹æ¡ˆï¼ˆstructured outputç‰ˆï¼‰"""
    llm = get_llm("planner", temperature=0.2)

    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶è¦æ±‚ã‚’æº€ãŸã™ãŸã‚ã®å®Ÿè¡Œè¨ˆç”»ã‚’ç«‹ã¦ã¾ã™ã€‚"
            "è¨ˆç”»ã¯å¿…ãš Python ã§å®Ÿè¡Œå¯èƒ½ãªå…·ä½“çš„ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€"
            "çŸ­ã„æ–‡ã®ãƒªã‚¹ãƒˆã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"
            "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ã‚¡ã‚¤ãƒ«æ›¸è¾¼ã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚"
        )
    )

    # planner ã«æ¸¡ã™å±¥æ­´ã¯éå¸¸ã«çŸ­ãï¼ˆéå»1ã‚¿ãƒ¼ãƒ³ç¨‹åº¦ï¼‰
    recent_msgs = get_recent_turn_messages(max_turns=1)
    decision: Plan = llm.with_structured_output(Plan).invoke(
        [system] + recent_msgs + [HumanMessage(content=state["user_input"])]
    )

    steps = decision.steps
    log = {"agent": "Planner", "output": "\n".join(f"- {s}" for s in steps)}

    return {**state, "plan": steps, "agent_logs": state["agent_logs"] + [log]}


def _extract_code_blocks(text: str) -> List[str]:
    """```python ... ``` ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºã€‚ãªã‘ã‚Œã°å…¨æ–‡ã‚’1ãƒ–ãƒ­ãƒƒã‚¯æ‰±ã„"""
    blocks = re.findall(r"```(?:python)?\s*([\s\S]*?)```", text, flags=re.IGNORECASE)
    if blocks:
        return [b.strip() for b in blocks if b.strip()]
    return [text.strip()] if text.strip() else []


def executor_node(
    state: AppState, ui_callback: Optional[Callable[[Dict[str, Any]], None]] = None
) -> AppState:
    """å®Ÿè¡Œè€…ï¼šå„ã‚¹ãƒ†ãƒƒãƒ—ã‚’Pythonã‚³ãƒ¼ãƒ‰åŒ–ã—ã€/tmp/ ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã£ã¦å®Ÿè¡Œ

    ui_callback ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆã‚’å³åº§ã«UIã«æ¸¡ã™ã€‚
    """
    llm = get_llm("executor", temperature=0.0)
    executions: List[str] = []

    tmp_dir = Path("tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)

    for i, step in enumerate(state.get("plan", []) or [], start=1):
        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’ Python ã‚³ãƒ¼ãƒ‰åŒ–
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ã‚ãªãŸã¯Pythonã‚³ãƒ¼ãƒ€ã§ã™ã€‚ä»¥ä¸‹ã®ã€å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã€ã‚’æº€ãŸã™æœ€å°ã®å®Ÿè¡Œå¯èƒ½ãªPythonã‚³ãƒ¼ãƒ‰ã‚’"
                    "1ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è¡¨ç¤ºã¯ print ã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚"
                    "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã¯ã—ãªã„ã§ãã ã•ã„ã€‚",
                ),
                ("human", "{step}"),
            ]
        )

        # executor ã«æ¸¡ã™å±¥æ­´: ã“ã‚Œã¾ã§ã®å®Ÿè¡Œçµæœï¼ˆã‚ã‚Œã°ï¼‰ã¨ç°¡æ½”ãªä¼šè©±å±¥æ­´
        recent_msgs = get_recent_turn_messages(max_turns=1)
        previous_execs_text = "\n\n".join(executions) if executions else ""
        extra_msgs: List[Any] = []
        if previous_execs_text:
            # å‰æ®µã®å®Ÿè¡Œãƒ­ã‚°ã‚’æ¸¡ã—ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã®æ•´åˆæ€§ã‚’ä¿ã¤
            extra_msgs.append(HumanMessage(content=f"ã“ã‚Œã¾ã§ã®å®Ÿè¡Œãƒ­ã‚°:\n{previous_execs_text}"))

        # æœ€å°é™ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ—ã‚’ä½œæˆ
        prompt_msgs = prompt.format_messages(step=step)
        messages = (
            [SystemMessage(content=prompt_msgs[0].content)]
            if prompt_msgs and isinstance(prompt_msgs[0], SystemMessage)
            else []
        )
        messages = (
            [
                SystemMessage(
                    content=(
                        "ã‚ãªãŸã¯Pythonã‚³ãƒ¼ãƒ€ã§ã™ã€‚ä»¥ä¸‹ã®ã€å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã€ã‚’æº€ãŸã™æœ€å°ã®å®Ÿè¡Œå¯èƒ½ãªPythonã‚³ãƒ¼ãƒ‰ã‚’"
                        "1ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è¡¨ç¤ºã¯ print ã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚"
                        "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã¯ã—ãªã„ã§ãã ã•ã„ã€‚"
                    )
                )
            ]
            + recent_msgs
            + extra_msgs
            + [HumanMessage(content=step)]
        )

        code_text = llm.invoke(messages).content
        code_blocks = _extract_code_blocks(code_text)

        if not code_blocks:
            exec_report = f"[Step {i}] ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            executions.append(exec_report)
            if ui_callback:
                ui_callback({"agent": f"Executor-Step-{i}", "output": exec_report})
            continue

        code = code_blocks[0]

        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãå‡ºã—
        file_path = tmp_dir / f"step_{i}.py"
        file_path.write_text(code, encoding="utf-8")

        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ
        try:
            result = subprocess.run(
                [sys.executable, str(file_path)],
                capture_output=True,
                text=True,
                timeout=20,
            )
            out = result.stdout.strip() or "(no output)"
            err = result.stderr.strip()
            exec_report = f"[Step {i}] OK\n--- file ---\n{file_path}\n--- code ---\n{code}\n--- output ---\n{out}"
            if err:
                exec_report += f"\n--- stderr ---\n{err}"
        except Exception as e:
            exec_report = (
                f"[Step {i}] å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\n--- file ---\n{file_path}\n--- code ---\n{code}"
            )

        executions.append(exec_report)

        # å„ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†æ™‚ã«UIã«å³æ™‚é€šçŸ¥
        if ui_callback:
            ui_callback({"agent": f"Executor-Step-{i}", "output": exec_report})

    log = {"agent": "Executor", "output": "\n\n".join(executions)}
    return {**state, "executions": executions, "agent_logs": state["agent_logs"] + [log]}


def summarizer_node(state: AppState) -> AppState:
    """å®Ÿè¡Œçµæœã®è¦ç´„"""
    llm = get_llm("summarizer", temperature=0.3)
    joined = "\n\n".join(state.get("executions") or [])

    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯ã‚µãƒãƒ©ã‚¤ã‚¶ã§ã™ã€‚ä»¥ä¸‹ã®å®Ÿè¡Œãƒ­ã‚°ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ã®è¦æ±‚ã«å¯¾ã™ã‚‹çµæœã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
            "å¿…è¦ãªã‚‰å®Ÿè¡Œå€¤ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )
    )

    # è¦ç´„ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®å…ƒã®è¦æ±‚ã‚’å¿µã®ãŸã‚æ¸¡ã™ï¼ˆæ–‡è„ˆç¢ºä¿ï¼‰
    recent_msgs = get_recent_turn_messages(max_turns=1)
    messages = (
        [system]
        + recent_msgs
        + [
            HumanMessage(content=joined),
            HumanMessage(content=f"å…ƒã®è¦æ±‚: {state.get('user_input','')}"),
        ]
    )

    # NOTE: ã‚‚ã—joinedãŒé•·å¤§ã«ãªã‚‹å ´åˆã¯è¦æ³¨æ„ï¼ˆæœ¬ãƒ‡ãƒ¢ã§ã¯çŸ­ã„ã¯ãšï¼‰
    answer = llm.invoke(messages).content
    log = {"agent": "Summarizer", "output": answer}
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


def responder_node(state: AppState) -> AppState:
    """Summarizer ã®è¦ç´„ã‚’ã‚‚ã¨ã«ã€ãƒ¦ãƒ¼ã‚¶å‘ã‘ã®æœ€çµ‚å›ç­”ã‚’è‡ªç„¶ãªæ–‡ç« ã«æ•´å½¢"""
    llm = get_llm("summarizer", temperature=0.5)  # summarizerã¨åŒã˜ã§ã‚‚è‰¯ã„ãŒåˆ‡æ›¿å¯
    system = SystemMessage(
        content=(
            "ã‚ãªãŸã¯å›ç­”è€…ã§ã™ã€‚ä»¥ä¸‹ã®è¦ç´„ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ã«ã‚ã‹ã‚Šã‚„ã™ãè‡ªç„¶ãªæ—¥æœ¬èªã§æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            "ä½™è¨ˆãªãƒ¡ã‚¿æƒ…å ±ï¼ˆå®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ãªã©ï¼‰ã¯å«ã‚ãšã€å¿…è¦ãªéƒ¨åˆ†ã ã‘ç°¡æ½”ã«ä¼ãˆã¦ãã ã•ã„ã€‚"
        )
    )

    # æœ€å°é™ã®å±¥æ­´ + è¦ç´„ã‚’æ¸¡ã™
    recent_msgs = get_recent_turn_messages(max_turns=1)
    messages = [system] + recent_msgs + [HumanMessage(content=state.get("answer", ""))]
    answer = llm.invoke(messages).content
    log = {"agent": "Responder", "output": answer}
    return {**state, "answer": answer, "agent_logs": state["agent_logs"] + [log]}


# =========================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (æ—¢å­˜ã®ãŸã‚ã«æ®‹ã™ãŒã€ä»Šå›ã¯ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œã®ãŸã‚ã«åˆ¥å®Ÿè£…ã‚’åˆ©ç”¨)
# =========================
def build_graph():
    g = StateGraph(AppState)

    g.add_node("router", router_node)
    g.add_node("chat_agent", chat_agent_node)
    g.add_node("planner", planner_node)
    g.add_node("executor", executor_node)
    g.add_node("summarizer", summarizer_node)
    g.add_node("responder", responder_node)

    g.set_entry_point("router")

    def route_decision(state: AppState):
        return state["mode"]

    g.add_conditional_edges(
        "router",
        route_decision,
        {
            "chat": "chat_agent",
            "code": "planner",
        },
    )

    g.add_edge("chat_agent", END)
    g.add_edge("planner", "executor")
    g.add_edge("executor", "summarizer")
    g.add_edge("summarizer", "responder")
    g.add_edge("responder", END)

    return g.compile()


GRAPH = build_graph()


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="LLM Multi-Agent Demo", page_icon="ğŸ¤–", layout="centered")
st.title("ğŸ¤– LLM Multi-Agent Demo (LangGraph + LangChain + Streamlit)")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šãƒ¡ãƒ¢ãƒª
if "history" not in st.session_state:
    st.session_state.history = []  # å„ã‚¿ãƒ¼ãƒ³ã® state ã‚’ä¿å­˜
if "thread_memory" not in st.session_state:
    # ãƒ¦ãƒ¼ã‚¶ç™ºè©±ã‚’å˜ç´”ã«é€£çµã§æŒã¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦é«˜åº¦ãªãƒ¡ãƒ¢ãƒªã«ç½®æ›å¯ï¼‰
    st.session_state.thread_memory = []


# UI ãƒ˜ãƒ«ãƒ‘: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤ºã™ã‚‹
def display_agent_log(container: st.delta_generator.DeltaGenerator, log: Dict[str, Any]):
    with container.expander(f"ğŸ§© {log.get('agent')}"):
        # å‡ºåŠ›ãŒ JSON-ish ãªå ´åˆã¯ st.json ã‚’ä½¿ã†ã¨è¦‹ã‚„ã™ã„
        out = log.get("output")
        notes = log.get("notes")
        if isinstance(out, (dict, list)):
            st.json(out)
        else:
            st.write(out)
        if notes:
            st.caption("notes:")
            st.json(notes)


# ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œé–¢æ•°: å„ãƒãƒ¼ãƒ‰ãŒçµ‚ã‚ã‚‹ãŸã³ã«UIã«è¡¨ç¤º
def run_graph_stepwise(
    init_state: AppState, ui_container: st.delta_generator.DeltaGenerator
) -> AppState:
    state = init_state

    # 1) Router
    state = router_node(state)
    # æœ€æ–°ãƒ­ã‚°ã‚’ UI ã«è¡¨ç¤º
    display_agent_log(ui_container, state["agent_logs"][-1])

    if state["mode"] == "chat":
        # Chat agent
        state = chat_agent_node(state)
        display_agent_log(ui_container, state["agent_logs"][-1])
        # å›ç­”ãŒã‚ã‚Œã°å³æ™‚è¡¨ç¤º
        if state.get("answer"):
            ui_container.success(state["answer"])
        return state

    # Code mode: Planner -> Executor (per-step) -> Summarizer -> Responder
    state = planner_node(state)
    display_agent_log(ui_container, state["agent_logs"][-1])

    # Executor: å„ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†æ™‚ã«å³æ™‚ UI ã¸æµã™ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™
    def executor_ui_callback(log: Dict[str, Any]):
        # ä¸€ã¤ãšã¤ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ã‚’ä½œã£ã¦å†…å®¹ã‚’å‡ºã™
        display_agent_log(ui_container, log)

    state = executor_node(state, ui_callback=executor_ui_callback)
    # Executor å…¨ä½“ã®ãƒ­ã‚°ï¼ˆã¾ã¨ã‚ï¼‰ã‚‚è¡¨ç¤º
    display_agent_log(ui_container, state["agent_logs"][-1])

    state = summarizer_node(state)
    display_agent_log(ui_container, state["agent_logs"][-1])

    state = responder_node(state)
    display_agent_log(ui_container, state["agent_logs"][-1])

    if state.get("answer"):
        ui_container.success(state["answer"])

    return state


with st.form(key="user_form", clear_on_submit=False):
    user_input = st.text_area(
        "æŒ‡ç¤ºæ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        height=160,
        placeholder="ä¾‹ï¼š2020ã€œ2024ã®ä¸–ç•ŒGDPä¸Šä½5ã‹å›½ã‚’ä¸€è¦§ã«ã—ã¦è€ƒå¯Ÿã—ã¦\nä¾‹ï¼šÏ€ã‚’100æ¡è¨ˆç®—ã—ã¦å…ˆé ­20æ¡ã ã‘è¡¨ç¤ºã—ã¦",
    )
    submitted = st.form_submit_button("é€ä¿¡")

if submitted and user_input.strip():
    # ã‚¹ãƒ¬ãƒƒãƒ‰è¨˜æ†¶ã‚’ä»˜åŠ ï¼ˆæœ¬ãƒ‡ãƒ¢ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã« Human ã®å±¥æ­´ã‚’æ¸¡ã™ã ã‘ï¼‰
    st.session_state.thread_memory.append(user_input.strip())

    init_state: AppState = {
        "user_input": user_input.strip(),
        "mode": "chat",
        "plan": None,
        "executions": None,
        "answer": None,
        "agent_logs": [],
    }

    # UI è¡¨ç¤ºé ˜åŸŸï¼ˆã“ã“ã«å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® expander ã‚’é †æ¬¡è¿½åŠ ã™ã‚‹ï¼‰
    ui_container = st.container()

    # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œï¼ˆå„ãƒãƒ¼ãƒ‰å®Œäº†æ™‚ã«å³æ™‚è¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
    result_state: AppState = run_graph_stepwise(init_state, ui_container)

    # å±¥æ­´ã«ä¿å­˜ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜æ†¶ä¿æŒï¼‰
    st.session_state.history.append(result_state)

# å±¥æ­´ã®è¡¨ç¤º
if st.session_state.history:
    st.subheader("ğŸ“š ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´")
    for idx, turn in enumerate(reversed(st.session_state.history), start=1):
        st.markdown(f"### Turn {len(st.session_state.history) - idx + 1}")
        for log in turn.get("agent_logs", []):
            with st.expander(f"ğŸ§© {log.get('agent')}"):
                st.write(log.get("output"))
                if "notes" in log and log["notes"]:
                    st.caption("notes:")
                    st.json(log["notes"])
        if turn.get("answer"):
            st.success(turn["answer"])
else:
    st.info("æœ€åˆã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


st.caption(
    "Tips: ãƒ«ãƒ¼ã‚¿ãŒã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã¨åˆ¤æ–­ã™ã‚‹ã¨ã€Planner â†’ Executor â†’ Summarizer ã®é †ã§å‹•ãã¾ã™ã€‚"
    "é€šå¸¸ä¼šè©±ã¨åˆ¤æ–­ã™ã‚‹ã¨ã€Webæ¤œç´¢ä»˜ãã® ChatAgent ãŒå¿œç­”ã—ã¾ã™ã€‚"
)
